{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNglEu-eDBi"
      },
      "source": [
        "# Dotrenowywanie modeli typu BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wWVJ-ooeDBe"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch] datasets\n",
        "!pip install -q wandb\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "YNFalamFh28E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "import wandb\n",
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "EqiJpcLt6oI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer)\n"
      ],
      "metadata": {
        "id": "VM5LyEBP-8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t6hrDSGeDBl"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yAkn3ureDBn"
      },
      "source": [
        "Ściągamy przygotowany wcześniej dataset [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):\n",
        "\n",
        "Składa się z tekstu i z labelki:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sry3_TBzeDBo"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "dataset[\"train\"][100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "bQwLfPGApAVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Żeby skrócić czas treningu tworzymy mniejszy dataset:"
      ],
      "metadata": {
        "id": "IqiQ954rtv7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_TRAIN = 1000\n",
        "N_VALID = 200\n",
        "N_TEST = 200\n",
        "\n",
        "small_train_set = dataset['train'].select([i for i in range(N_TRAIN)])\n",
        "small_valid_set = dataset['train'].select([5000+i for i in range(N_VALID)])\n",
        "small_test_set = dataset['test'].select([i for i in range(N_VALID)])\n",
        "\n",
        "small_dataset = DatasetDict({\n",
        "    'train': small_train_set,\n",
        "    'test': small_test_set,\n",
        "    'valid': small_valid_set})"
      ],
      "metadata": {
        "id": "IoU8o4wAokm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset)"
      ],
      "metadata": {
        "id": "qL-Qh75Mqg73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXicB-peDBo"
      },
      "source": [
        "Mamy tekst i labelki, ale teraz musimy jeszcze przygotować dane do przetwarzania przez transformery:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzMdiC0geDBp"
      },
      "outputs": [],
      "source": [
        "max_length = 512\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "\n",
        "tokenized_datasets = small_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"Where are we now? How many tokens are in seqence?\"\n",
        "\n",
        "bert_tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "\n",
        "\n",
        "print(\"BERT:\", bert_tokenized_sequence)"
      ],
      "metadata": {
        "id": "Bm4EFBlluzas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ow0LC1-eDBr"
      },
      "source": [
        "## Trening"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBdrOy4eDBr"
      },
      "source": [
        "## Trening z wbudowaną klasą Trainer\n",
        "\n",
        "Teraz pobieramy pretrenowany model z huggingface i określamy ile jest docelowych klas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx_ZjdLgeDBs"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJAY3MweDBt"
      },
      "source": [
        "### Ewaluacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzInmhRGeDBt"
      },
      "source": [
        "W trakcie treningu automatycznie liczymy tylko funkcje straty; jeśli chcemy mieć dodatkowe metryki musimy je dodać sami, na początek zaimplementujmy tylko dokładność"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycbK1kQeDBt"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tworzymy metodę do liczenia dokładności:"
      ],
      "metadata": {
        "id": "emDYh7Ze_z0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyhN63s9eDBt"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzqAJp0IeDBs"
      },
      "source": [
        "### Hiperparametery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6l8nkkVeDBu"
      },
      "source": [
        "Teraz definiujemy parametry treningu - w zależności jak trafnie je dobierzemy, tym lepsze uzyskamy wyniki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrzaWNWteDBu"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  num_train_epochs=3,\n",
        "                                  learning_rate=0.00002,\n",
        "                                  per_device_train_batch_size=8,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  weight_decay=0.001,\n",
        "                                  disable_tqdm=False,\n",
        "                                  overwrite_output_dir=True,\n",
        "                                  metric_for_best_model='eval_loss',\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  save_strategy='epoch',\n",
        "                                  logging_strategy='epoch',\n",
        "                                  log_level='error',\n",
        "                                  warmup_ratio=0.05,\n",
        "                                  report_to='wandb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlU38s6eDBu"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1glTdS1teDBu"
      },
      "source": [
        "Huggingface używa osobnego obiektu, służącemu do treningu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QPgzQCWeDBu"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['valid'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL6etSUBeDBv"
      },
      "source": [
        "Dotrenowanie modelu uruchamiamy przy użyciu [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJD6QFTzeDBv"
      },
      "outputs": [],
      "source": [
        "wandb.init(project='distilber_reviews')\n",
        "trainer.train()\n",
        "preds_output = trainer.predict(tokenized_datasets['test'])\n",
        "print(f\"Test accuracy was {preds_output.metrics['test_accuracy']*100}%\")\n",
        "wandb_di = {'test_acc': preds_output.metrics['test_accuracy']}\n",
        "wandb.log(wandb_di)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizacja różnych modeli:"
      ],
      "metadata": {
        "id": "K8vzkHoosXuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "JSa40yPdsbD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7sUvo8h24BaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"Where are we now? Can we find rules used for tokenization, especially longer words such as honorificabilitudinitatibus?\"\n",
        "\n",
        "bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)\n",
        "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
        "\n",
        "print(\"BERT:\", bert_tokenized_sequence)\n",
        "print(\"RoBERTa:\", roberta_tokenized_sequence)"
      ],
      "metadata": {
        "id": "U0ng0mJksm4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline i NER\n",
        "\n",
        "Trenowanie NERa to tak naprawdę clasyfikacja każdego tokenu:\n",
        "\n",
        "![](https://ar5iv.labs.arxiv.org/html/1912.01389/assets/bert_arch.png)\n",
        "\n",
        "Przykład, jak trenuje się takie datasety znajduje się [tutaj](https://huggingface.co/learn/nlp-course/chapter7/2)\n",
        "\n",
        "\n",
        "Warto pamiętać, że można wykorzystać już wcześniej wytrenowane obiekty, jeśli użyjmey pipeline'u, sprawa staję się banalnie prosta.\n"
      ],
      "metadata": {
        "id": "vEhoqhhb9yiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "model_checkpoint = \"pietruszkowiec/herbert-base-ner\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"Nazywam się Grzegorz Brzęszczyszczykiewicz, pochodzę \"\\\n",
        "    \"z Chrząszczyżewoszczyc, pracuję w Łękołodzkim Urzędzie Powiatowym\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "results_df = pd.DataFrame(ner_results)\n",
        "results_df.head(20)\n"
      ],
      "metadata": {
        "id": "rsruUpBvvgjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA\n",
        "\n",
        "Przy pomocy architektury BERT QA jest dość prymitywny, ale zobaczmy jak działa:"
      ],
      "metadata": {
        "id": "Mj2_wlZSyNN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/deberta-v3-large-squad2\"\n",
        "\n",
        "# a) Get predictions\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "QA_input = {\n",
        "    'question': 'Why is model conversion important?',\n",
        "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
        "}\n",
        "res = nlp(QA_input)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "aMjWp6pyyM_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie\n",
        "\n",
        "Znaleźć wytrenowany model do:\n",
        "- klasyfikacji wydźwięku twittera w huggingface, wgrać go i sprawdzić jak działa na wybranych tekstach, można porównać kilka modeli\n",
        "- do klasyfikacji toskycznych komentarzy w sieci i sprawdzić jak działa."
      ],
      "metadata": {
        "id": "aXzZs-RMw_rI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-BMIH10wyLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}